# import necessary packages
from helpers import HDF5DatasetWriter
from imutils import paths
from scipy import misc
import argparse
import shutil
import random
import cv2
import os

ap = argparse.ArgumentParser()
ap.add_argument("-i", "--input-images", type=str, required=True,
    help="path to input images to process")
ap.add_argument("-l", "--labels", type=str, required=True,
    help="path to output labels")
ap.add_argument("-o", "--output-images", type=str, required=True,
    help="path to output images")
ap.add_argument("-b", "--input-db", type=str, required=True,
    help="path to input database")
ap.add_argument("-c", "--output-db", type=str, required=True,
    help="path to output database")
ap.add_argument("-s", "--scale", type=float, default=2.0,
    help="scale factor for images")
ap.add_argument("-d", "--input-dim", type=int, default=33,
    help="scale factor for images")
ap.add_argument("-t", "--stride", type=int, default=14,
    help="scale factor for images")
ap.add_argument("-p", "--padding", type=int, default=5,
    help="scale factor for images")
ap.add_argument("-a", "--label-size", type=int, default=21,
    help="scale factor for images")
args = vars(ap.parse_args())

# if the output directories do not exist, create them
for p in [args["output_images"], args["labels"]]:
    if not os.path.exists(p):
        os.makedirs(p)

# grab the image paths and initialize the total number
# of crops processed
print("[INFO] creating temporary images")
imagePaths = list(paths.list_images(args["input_images"]))
random.shuffle(imagePaths)
total = 0

# loop over the image paths
for path in imagePaths:
    # load the input image
    image = cv2.imread(path)

    # grab the dimensions of the input image and crop the image
    # such that it tiles nicely when generate training dataset
    # and labels
    (h, w) = image.shape[:2]
    w -= int(w % args["scale"])
    h -= int(h % args["scale"])
    image = image[0:h, 0:w]

    # training images are generated by downsampling images
    # and then upscaling them
    scaled = misc.imresize(image, 1.0 / args["scale"],
        interp="bicubic")
    scaled = misc.imresize(image, args["scale"] / 1.0,
        interp="bicubic")

    # slide a window from left to right and top to bottom
    for y in range(0, h - args["input_dim"] + 1, args["stride"]):
        for x in range(0, w - args["input_dim"] + 1, args["stride"]):
            # crop output the INPUT_DIM*INPUT_DIM ROI from
            # the scaled image (it will be the net input)
            crop = scaled[y:y + args["input_dim"],
                x:x + args["input_dim"]]

            # crop the LABEL_SIZE*LABEL_SIZE ROI
            # will be the network target
            target = image[
                y + args["padding"]:y + args["padding"] + args["label_size"],
                x + args["padding"]:x + args["padding"] + args["label_size"]
            ]

            # construct the crop and target output image paths
            cropPath = os.path.sep.join([args["output_images"],
                "{}.png".format(total)])
            targetPath = os.path.sep.join([args["labels"],
                "{}.png".format(total)])

            # write the image to disk
            cv2.imwrite(cropPath, crop)
            cv2.imwrite(targetPath, target)

            # increment the total number of crop
            total += 1

print("[INFO] building HDF5 datasets...")
inputPaths = sorted(list(paths.list_images(args["output_images"])))
outputPaths = sorted(list(paths.list_images(args["labels"])))

# initialize the HDF5 DATASETS
inputWriter = HDF5DatasetWriter((len(inputPaths), args["input_dim"],
    args["input_dim"], 3), args["input_db"])
outputWriter = HDF5DatasetWriter((len(outputPaths),
    args["label_size"], args["label_size"], 3), args["output_db"])

# loop over the images
for (inputPath, outputPath) in zip(inputPaths, outputPaths):
    # load the two images and add them to ther respective datasets
    inputImage = cv2.imread(inputPath)
    outputImage = cv2.imread(outputPath)
    inputWriter.add([inputImage], [-1])
    outputWriter.add([outputImage], [-1])

# close the HDF5 datasets
inputWriter.close()
outputWriter.close()

# delete the temporary files
print("[INFO] cleaning up directory...")
shutil.rmtree(args["output_images"])
shutil.rmtree(args["labels"])
